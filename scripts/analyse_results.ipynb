{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(font=\"Helvetica\")\n",
    "ROOT = os.getcwd().split(\"scripts\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditioning(row):\n",
    "    \"\"\"\n",
    "    Returns a string describing the conditioning used for a given experiment.\n",
    "    \"\"\"\n",
    "    conditioning = []\n",
    "    if row[\"use_rtg\"]:\n",
    "        conditioning.append(\"rtg\")\n",
    "    if row[\"use_mission\"]:\n",
    "        conditioning.append(\"mission\")\n",
    "    if row[\"use_feedback\"]:\n",
    "        if row[\"feedback_mode\"] != \"all\":\n",
    "            conditioning.append(f\"{row['feedback_mode']} feedback\")\n",
    "        else:\n",
    "            conditioning.append(\"all feedback\")\n",
    "    return \" + \".join(conditioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiments(output_path, dir):\n",
    "    \"\"\"\n",
    "    Returns a list of dataframes containing the results of all experiments in a given directory.\n",
    "    \"\"\"\n",
    "    settings = dir.split(\"-\")[5:]\n",
    "    dfs = []\n",
    "    for seed_dir in os.listdir(os.path.join(output_path, dir)):\n",
    "        exp_path = os.path.join(output_path, os.path.join(dir, seed_dir))\n",
    "        try:\n",
    "            df = pd.read_pickle(os.path.join(exp_path, \"results.pkl\"))\n",
    "            df[\"model_seed\"] = seed_dir\n",
    "        except:\n",
    "            continue\n",
    "        for s in settings:\n",
    "            key = s.split(\"_\")[:-1]\n",
    "            key = \"_\".join(key)\n",
    "            value = s.split(\"_\")[-1]\n",
    "            value = (\n",
    "                int(value)\n",
    "                if value.isnumeric()\n",
    "                else (\n",
    "                    True if value == \"True\" else (False if value == \"False\" else value)\n",
    "                )\n",
    "            )\n",
    "            df[key] = value\n",
    "        try:\n",
    "            df[\"conditioning\"] = df.apply(lambda row: get_conditioning(row), axis=1)\n",
    "        except KeyError:\n",
    "            raise KeyError(\"Could not generate conditioning column\")\n",
    "        dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_df(output_path, level=None):\n",
    "    \"\"\"\n",
    "    Returns a dataframe containing the results of all experiments in a given directory.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for dir in os.listdir(output_path):\n",
    "        if \"level\" in dir:\n",
    "            if level and level.lower() not in dir:\n",
    "                continue\n",
    "            current_dfs = get_experiments(output_path, dir)\n",
    "            dfs.extend(current_dfs)\n",
    "    dfs = [df[(df[\"eval_type\"] != \"efficiency\") & (df[\"model\"] == \"DT\")] for df in dfs]\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggr_results(df, inference_mode, eval_type, metric):\n",
    "    \"\"\"\n",
    "    Returns a dataframe containing the aggregated results of a given evaluation type.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_results = df[\n",
    "        (df[\"eval_type\"] == eval_type)\n",
    "        & (\n",
    "            (df[\"feedback_at_inference\"] == inference_mode)\n",
    "            | (pd.isna(df[\"feedback_at_inference\"]))\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    results = (\n",
    "        filtered_results[[\"conditioning\", metric]]\n",
    "        .groupby([\"conditioning\"])\n",
    "        .agg(\n",
    "            Mean=(metric, \"mean\"),\n",
    "            Std=(metric, \"std\"),\n",
    "            Min=(metric, \"min\"),\n",
    "            Max=(metric, \"max\"),\n",
    "        )\n",
    "    )\n",
    "    results[\"Mean\"] = results[\"Mean\"].round(4)\n",
    "    results[\"Std\"] = results[\"Std\"].round(4)\n",
    "\n",
    "    results[\"ood_type\"] = \"all types\" if eval_type == \"ood_generalisation\" else None\n",
    "    results[\"eval_type\"] = eval_type\n",
    "    results[\"feedback_at_inference\"] = inference_mode\n",
    "\n",
    "    if eval_type == \"ood_generalisation\":\n",
    "        results_by_ood = (\n",
    "            filtered_results[[\"ood_type\", \"conditioning\", metric]]\n",
    "            .groupby([\"ood_type\", \"conditioning\"])\n",
    "            .agg(\n",
    "                Mean=(metric, \"mean\"),\n",
    "                Std=(metric, \"std\"),\n",
    "                Min=(metric, \"min\"),\n",
    "                Max=(metric, \"max\"),\n",
    "            )\n",
    "        )\n",
    "        results_by_ood[\"Mean\"] = results_by_ood[\"Mean\"].round(4)\n",
    "        results_by_ood[\"Std\"] = results_by_ood[\"Std\"].round(4)\n",
    "\n",
    "        results_by_ood[\"eval_type\"] = eval_type\n",
    "        results_by_ood[\"feedback_at_inference\"] = inference_mode\n",
    "    else:\n",
    "        results_by_ood = None\n",
    "\n",
    "    return results, results_by_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deltas(df, reference):\n",
    "    \"\"\"\n",
    "    Returns a dataframe containing the deltas of a given dataframe with respect to a reference (mission or RTG).\n",
    "    \"\"\"\n",
    "    delta_df = df[\n",
    "        (df[\"conditioning\"].str.contains(reference))\n",
    "        | (df[\"conditioning\"] == \"all feedback\")\n",
    "    ]\n",
    "    delta_df[\"Delta (Mean)\"] = delta_df.apply(\n",
    "        lambda row: row[\"Mean\"]\n",
    "        - delta_df.loc[delta_df[\"conditioning\"] == reference, \"Mean\"].values[0],\n",
    "        axis=1,\n",
    "    )\n",
    "    delta_df[\"Delta (Mean)\"] = delta_df[\"Delta (Mean)\"].round(4)\n",
    "    delta_df[\"reference\"] = reference\n",
    "    return delta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(df, metric):\n",
    "    \"\"\"\n",
    "    Returns a dataframe containing the aggregated results of all evaluation types.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for reference in [\"mission\", \"rtg\"]:\n",
    "        for inference_mode in df[\"feedback_at_inference\"].unique():\n",
    "            if pd.isna(inference_mode):\n",
    "                continue\n",
    "            for eval_type in df[\"eval_type\"].unique():\n",
    "                results, results_by_ood = aggr_results(\n",
    "                    df, inference_mode, eval_type, metric\n",
    "                )\n",
    "                results.reset_index(inplace=True)\n",
    "\n",
    "                results_with_deltas = get_deltas(results, reference)\n",
    "                dfs.append(results_with_deltas)\n",
    "\n",
    "                if results_by_ood is not None:\n",
    "                    results_by_ood.reset_index(inplace=True)\n",
    "                    for ood_type in results_by_ood[\"ood_type\"].unique():\n",
    "                        results_by_ood_with_deltas = get_deltas(\n",
    "                            results_by_ood[results_by_ood[\"ood_type\"] == ood_type],\n",
    "                            reference,\n",
    "                        )\n",
    "                        dfs.append(results_by_ood_with_deltas)\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_as_csv(df, level, metric, output_path):\n",
    "    \"\"\"\n",
    "    Saves a dataframe as a csv file.\n",
    "    \"\"\"\n",
    "    path = os.path.join(output_path, f\"results_{level}_{metric}.csv\")\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_colors(df, colors):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping conditioning variants to colors for plots.\n",
    "    \"\"\"\n",
    "    labels = list(df[\"conditioning\"].unique())\n",
    "    return {\n",
    "        label: colors[\"uoe_si_colors\"][i + 1]\n",
    "        if \"mission\" in label\n",
    "        else (\n",
    "            colors[\"hwu_colors\"][0]\n",
    "            if label == \"all feedback\"\n",
    "            else colors[\"aarg_colors\"][i]\n",
    "        )\n",
    "        for i, label in enumerate(labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axis_ticks(ax, reference, reference_perf, ylims, sizing_config):\n",
    "    \"\"\"\n",
    "    Sets the reference performance on the y-axis (instead of 0).\n",
    "    \"\"\"\n",
    "    ax.set_ylim(bottom=-ylims, top=ylims)\n",
    "    yticklabels = ax.get_yticks().tolist()\n",
    "\n",
    "    new_yticklabels = [\n",
    "        f\"{reference_perf*100:.2f}*\"\n",
    "        if float(ytick) == float(0)\n",
    "        else (f\"+{ytick*100:.2f}\" if float(ytick) > float(0) else f\"{ytick*100:.2f}\")\n",
    "        for ytick in yticklabels\n",
    "    ]\n",
    "    ax.set_yticklabels(new_yticklabels, fontsize=sizing_config[\"tick_label_size\"])\n",
    "    xticklabels = ax.get_xticklabels()\n",
    "    new_xticklabels = [\n",
    "        f\"vs {label.get_text()}\".replace(f\"vs {reference}\", \"\").replace(\n",
    "            \"feedback\", \"\\nfeedback\"\n",
    "        )\n",
    "        for label in xticklabels\n",
    "    ]\n",
    "    ax.set_xticklabels(new_xticklabels, fontsize=sizing_config[\"tick_label_size\"])\n",
    "    plt.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bar_values(ax, df, ylims, sizing_config):\n",
    "    \"\"\"\n",
    "    Sets the values of the bars on top of the bars (for positive deltas)\n",
    "    / below the bars (for negatvie deltas).\n",
    "    \"\"\"\n",
    "    for index, value in enumerate(df[\"Delta (Mean)\"]):\n",
    "        x_pos = index\n",
    "        y_pos = value + ylims / 25 if value > 0 else value - ylims / 25\n",
    "        va = \"bottom\" if value > 0 else \"top\"\n",
    "        ax.text(\n",
    "            x_pos,\n",
    "            y_pos,\n",
    "            f\"{value*100:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=va,\n",
    "            fontsize=sizing_config[\"value_size\"],\n",
    "        )\n",
    "    plt.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pad_title(level, eval_type, inference_mode, sizing_config):\n",
    "    \"\"\"\n",
    "    Sets the title of the plot, with padding.\n",
    "    \"\"\"\n",
    "    if \"all\" in level.lower() or \"single\" in level.lower() or \"maze\" in level.lower():\n",
    "        level = level\n",
    "    else:\n",
    "        level = \"level \" + f\"'{level}'\"\n",
    "    plt.title(\n",
    "        f\"{eval_type.replace('_', ' ')} on {level} \\n({inference_mode} feedback at inference)\",\n",
    "        fontsize=sizing_config[\"title_size\"],\n",
    "    )\n",
    "    plt.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pad_axis_labels(ax, metric, reference, sizing_config):\n",
    "    \"\"\"\n",
    "    Sets the labels of the x and y axes, with padding.\n",
    "    \"\"\"\n",
    "    ax.set_xlabel(\n",
    "        \"Conditioning variant\",\n",
    "        fontsize=sizing_config[\"axis_label_size\"],\n",
    "        wrap=True,\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        f\"Î” {metric.replace('_', ' ')}{' rate' if metric == 'gc_success' else ''}\\n*{reference}\",\n",
    "        fontsize=sizing_config[\"axis_label_size\"],\n",
    "        labelpad=sizing_config[\"axis_label_size\"] * 0.75,\n",
    "        wrap=True,\n",
    "    )\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", pad=-2)\n",
    "    plt.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_config(size):\n",
    "    figma_dpi = 72\n",
    "    small_height = 132 / figma_dpi\n",
    "    medium_height = 198 / figma_dpi\n",
    "    large_height = 396 / figma_dpi\n",
    "    full_width = 396 / figma_dpi\n",
    "    half_width = 190.5 / figma_dpi\n",
    "    third_width = 122 / figma_dpi\n",
    "\n",
    "    xsmall_font_size = 6\n",
    "    small_font_size = 9\n",
    "    medium_font_size = 10\n",
    "    large_font_size = 14\n",
    "    xlarge_font_size = 16\n",
    "\n",
    "    xsmall_font_set = (\n",
    "        xsmall_font_size,\n",
    "        xsmall_font_size,\n",
    "        xsmall_font_size,\n",
    "        xsmall_font_size,\n",
    "    )\n",
    "\n",
    "    small_font_set = (\n",
    "        small_font_size,\n",
    "        small_font_size,\n",
    "        xsmall_font_size,\n",
    "        xsmall_font_size,\n",
    "    )\n",
    "    medium_font_set = (\n",
    "        large_font_size,\n",
    "        medium_font_size,\n",
    "        small_font_size,\n",
    "        xsmall_font_size,\n",
    "    )\n",
    "    large_font_set = (\n",
    "        xlarge_font_size,\n",
    "        large_font_size,\n",
    "        medium_font_size,\n",
    "        small_font_size,\n",
    "    )\n",
    "\n",
    "    if size == \"small-third\":\n",
    "        figsize = (third_width, small_height)\n",
    "        font_sizes = xsmall_font_set\n",
    "    elif size == \"small-half\":\n",
    "        figsize = (half_width, small_height)\n",
    "        font_sizes = xsmall_font_set\n",
    "    elif size == \"small\":\n",
    "        figsize = (full_width, small_height)\n",
    "        font_sizes = small_font_set\n",
    "    elif size == \"medium-third\":\n",
    "        figsize = (third_width, medium_height)\n",
    "        font_sizes = xsmall_font_set\n",
    "    elif size == \"medium-half\":\n",
    "        figsize = (half_width, medium_height)\n",
    "        font_sizes = small_font_set\n",
    "    elif size == \"medium\":\n",
    "        figsize = (full_width, medium_height)\n",
    "        font_sizes = medium_font_set\n",
    "    elif size == \"large-third\":\n",
    "        figsize = (third_width, large_height)\n",
    "        font_sizes = medium_font_set\n",
    "    elif size == \"large-half\":\n",
    "        figsize = (half_width, large_height)\n",
    "        font_sizes = large_font_set\n",
    "    else:\n",
    "        figsize = (full_width, large_height)\n",
    "        font_sizes = large_font_set\n",
    "\n",
    "    return {\n",
    "        \"figsize\": figsize,\n",
    "        \"title_size\": font_sizes[0],\n",
    "        \"axis_label_size\": font_sizes[1],\n",
    "        \"tick_label_size\": font_sizes[2],\n",
    "        \"value_size\": font_sizes[3],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deltas_without_text(\n",
    "    sizing_config,\n",
    "    df,\n",
    "    ylims,\n",
    "    color_palette,\n",
    "):\n",
    "    fig1 = plt.figure(figsize=sizing_config[\"figsize\"])\n",
    "    ax1 = plt.subplot()\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"conditioning\",\n",
    "        y=\"Delta (Mean)\",\n",
    "        ax=ax1,\n",
    "        errorbar=None,\n",
    "        palette=color_palette,\n",
    "    )\n",
    "    plt.title(\n",
    "        \" \\n \",\n",
    "        fontsize=sizing_config[\"title_size\"],\n",
    "    )\n",
    "\n",
    "    ax1.set_ylim(bottom=-ylims, top=ylims)\n",
    "\n",
    "    ax1.set_xlabel(\" \", fontsize=sizing_config[\"axis_label_size\"])\n",
    "    ax1.set_xticklabels([])\n",
    "\n",
    "    ax1.set_ylabel(\n",
    "        \" \\n \",\n",
    "        fontsize=sizing_config[\"axis_label_size\"],\n",
    "        labelpad=sizing_config[\"axis_label_size\"] * 2,\n",
    "    )\n",
    "    ax1.set_yticklabels([])\n",
    "\n",
    "    plt.margins(x=0.025, y=0.025)\n",
    "    plt.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deltas_with_text(\n",
    "    sizing_config,\n",
    "    df,\n",
    "    level,\n",
    "    metric,\n",
    "    ylims,\n",
    "    eval_type,\n",
    "    reference,\n",
    "    reference_perf,\n",
    "    inference_mode,\n",
    "    color_palette,\n",
    "):\n",
    "    fig2 = plt.figure(figsize=sizing_config[\"figsize\"])\n",
    "    ax2 = plt.subplot()\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"conditioning\",\n",
    "        y=\"Delta (Mean)\",\n",
    "        ax=ax2,\n",
    "        errorbar=None,\n",
    "        palette=color_palette,\n",
    "    )\n",
    "    set_axis_ticks(ax2, reference, reference_perf, ylims, sizing_config)\n",
    "    set_bar_values(ax2, df, ylims, sizing_config)\n",
    "    set_pad_title(level, eval_type, inference_mode, sizing_config)\n",
    "    set_pad_axis_labels(ax2, metric, reference, sizing_config)\n",
    "    plt.margins(x=0.025, y=0.025)\n",
    "    plt.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deltas(\n",
    "    df,\n",
    "    level,\n",
    "    metric,\n",
    "    colors,\n",
    "    size,\n",
    "    ylims,\n",
    "    eval_type,\n",
    "    output_path,\n",
    "    reference,\n",
    "    reference_perf,\n",
    "    inference_mode,\n",
    "    ood_type=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the deltas of a given dataframe with respect to a reference (mission or RTG).\n",
    "    \"\"\"\n",
    "    color_palette = map_colors(df, colors)\n",
    "    sizing_config = get_plot_config(size)\n",
    "    eval_type = f\"{eval_type.split('_')[0].upper()} generalisation{f' ({ood_type})' if ood_type else ''}\"\n",
    "    plot_deltas_with_text(\n",
    "        sizing_config,\n",
    "        df,\n",
    "        level,\n",
    "        metric,\n",
    "        ylims,\n",
    "        eval_type,\n",
    "        reference,\n",
    "        reference_perf,\n",
    "        inference_mode,\n",
    "        color_palette,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        os.path.join(\n",
    "            output_path,\n",
    "            f\"{level}_{metric}_{eval_type}_{reference}_{inference_mode}.png\",\n",
    "        ),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.close()\n",
    "    plot_deltas_without_text(sizing_config, df, ylims, color_palette)\n",
    "    plt.savefig(\n",
    "        os.path.join(\n",
    "            output_path,\n",
    "            f\"{level}_{metric}_{eval_type}_{reference}_{inference_mode}_no_text.png\",\n",
    "        ),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, level, metric, colors, size, output_path):\n",
    "    \"\"\"\n",
    "    Plots the results of a given dataframe.\n",
    "    \"\"\"\n",
    "    ylims = max(abs(df[\"Delta (Mean)\"].max()), abs(df[\"Delta (Mean)\"].min())) * (\n",
    "        1.25 if not \"all\" in level else 1.05\n",
    "    )\n",
    "    for reference in [\"mission\", \"rtg\"]:\n",
    "        for eval_type in df[\"eval_type\"].unique():\n",
    "            for inference_mode in df[\"feedback_at_inference\"].unique():\n",
    "                if pd.isna(inference_mode):\n",
    "                    continue\n",
    "\n",
    "                for ood_type in df[df[\"eval_type\"] == eval_type][\"ood_type\"].unique():\n",
    "                    if pd.isna(ood_type):\n",
    "                        results_with_ref = df[\n",
    "                            (df[\"eval_type\"] == eval_type)\n",
    "                            & (df[\"reference\"] == reference)\n",
    "                            & (pd.isna(df[\"ood_type\"]))\n",
    "                            & (\n",
    "                                (df[\"feedback_at_inference\"] == inference_mode)\n",
    "                                | (pd.isna(df[\"feedback_at_inference\"]))\n",
    "                            )\n",
    "                        ]\n",
    "                    else:\n",
    "                        results_with_ref = df[\n",
    "                            (df[\"eval_type\"] == eval_type)\n",
    "                            & (df[\"reference\"] == reference)\n",
    "                            & (df[\"ood_type\"] == ood_type)\n",
    "                            & (\n",
    "                                (df[\"feedback_at_inference\"] == inference_mode)\n",
    "                                | (pd.isna(df[\"feedback_at_inference\"]))\n",
    "                            )\n",
    "                        ]\n",
    "\n",
    "                    reference_perf = results_with_ref[\n",
    "                        results_with_ref[\"conditioning\"] == reference\n",
    "                    ][\"Mean\"].values[0]\n",
    "                    results = results_with_ref[\n",
    "                        ~df[\"conditioning\"].isin([\"mission\", \"rtg\"])\n",
    "                    ]\n",
    "                    results.sort_values(\"conditioning\", inplace=True)\n",
    "\n",
    "                    plot_deltas(\n",
    "                        results,\n",
    "                        level,\n",
    "                        metric,\n",
    "                        colors,\n",
    "                        size,\n",
    "                        ylims,\n",
    "                        eval_type,\n",
    "                        output_path,\n",
    "                        reference,\n",
    "                        reference_perf,\n",
    "                        inference_mode,\n",
    "                        ood_type,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_mode_diffs(results_df, level):\n",
    "    x = results_df.sort_values(\n",
    "        by=[\n",
    "            \"reference\",\n",
    "            \"conditioning\",\n",
    "            \"eval_type\",\n",
    "            \"ood_type\",\n",
    "            \"feedback_at_inference\",\n",
    "        ],\n",
    "        ascending=False,\n",
    "    )\n",
    "    x[\"diff\"] = x[\"Delta (Mean)\"].diff()\n",
    "    x[\"diff\"] = x[\"diff\"].apply(lambda x: round(x, 4))\n",
    "    x[\"diff\"][x[\"feedback_at_inference\"] == \"numerical\"] = 0\n",
    "    inference_mode_diffs = x[\n",
    "        [\n",
    "            \"conditioning\",\n",
    "            \"eval_type\",\n",
    "            \"ood_type\",\n",
    "            \"diff\",\n",
    "        ]\n",
    "    ][\n",
    "        (~x[\"conditioning\"].isin([\"mission\", \"rtg\"]))\n",
    "        & (x[\"feedback_at_inference\"] != \"numerical\")\n",
    "    ]\n",
    "    inference_mode_diffs_new = inference_mode_diffs.merge(\n",
    "        results_df[\n",
    "            [\"conditioning\", \"eval_type\", \"ood_type\", \"reference\", \"Delta (Mean)\"]\n",
    "        ][results_df[\"feedback_at_inference\"] == \"numerical\"],\n",
    "        on=[\"conditioning\", \"eval_type\", \"ood_type\"],\n",
    "    ).drop_duplicates()\n",
    "    inference_mode_diffs_new[\"level\"] = level\n",
    "    inference_mode_diffs_rtg = inference_mode_diffs_new[\n",
    "        inference_mode_diffs_new[\"reference\"] == \"rtg\"\n",
    "    ]\n",
    "    inference_mode_diffs_mission = inference_mode_diffs_new[\n",
    "        inference_mode_diffs_new[\"reference\"] == \"mission\"\n",
    "    ]\n",
    "    return inference_mode_diffs_rtg, inference_mode_diffs_mission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diffs_vs_numerical_performance(df, level, output_path):\n",
    "    \"\"\"\n",
    "    Plot correllation between differences in performance of conditioning variants with actual feedback at inference and numerical performance.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"Delta (Mean)\",\n",
    "        y=\"diff\",\n",
    "        hue=\"conditioning\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    for c in df[\"conditioning\"].unique():\n",
    "        cdf = df[df[\"conditioning\"] == c]\n",
    "        m, b = np.polyfit(cdf[\"Delta (Mean)\"], cdf[\"diff\"], 1)\n",
    "        plt.plot(cdf[\"Delta (Mean)\"], m * cdf[\"Delta (Mean)\"] + b)\n",
    "    plt.savefig(\n",
    "        os.path.join(output_path, f\"{level}_diffs_vs_numerical_performance.png\"),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(df, level, metric, colors, size, output_path):\n",
    "    print(level.upper())\n",
    "    if \"single\" in level.lower():\n",
    "        filtered_df = df[\n",
    "            df[\"level\"].isin([\"GoToObj\", \"GoToLocal\", \"PutNextLocal\", \"PickupLoc\"])\n",
    "        ]\n",
    "    elif \"maze\" in level.lower():\n",
    "        filtered_df = df[\n",
    "           ~df[\"level\"].isin([\"GoToObj\", \"GoToLocal\", \"PutNextLocal\", \"PickupLoc\"])\n",
    "        ]\n",
    "    elif \"all\" in level.lower():\n",
    "        filtered_df = df\n",
    "    else:\n",
    "        filtered_df = df[df[\"level\"] == level]\n",
    "    results = combine_results(filtered_df, metric)\n",
    "    inference_mode_diffs_rtg, inference_mode_diffs_mission = get_inference_mode_diffs(\n",
    "        results, level\n",
    "    )\n",
    "    save_results_as_csv(results, level, metric, output_path)\n",
    "    save_results_as_csv(\n",
    "        inference_mode_diffs_rtg, level, \"inference_mode_diff_rtg\", output_path\n",
    "    )\n",
    "    save_results_as_csv(\n",
    "        inference_mode_diffs_mission,\n",
    "        level,\n",
    "        \"inference_mode_diff_mission\",\n",
    "        output_path,\n",
    "    )\n",
    "    plot_results(results, level, metric, colors, size, output_path)\n",
    "    plot_diffs_vs_numerical_performance(inference_mode_diffs_rtg, level, output_path)\n",
    "    plot_diffs_vs_numerical_performance(inference_mode_diffs_mission, level, output_path)\n",
    "    print(\"Done!\")\n",
    "    return inference_mode_diffs_rtg, inference_mode_diffs_mission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_inference_mod_diffs(df, output_path):\n",
    "    all_inference_mode_diffs = (\n",
    "        pd.concat(df)\n",
    "        .sort_values(\n",
    "            by=[\"conditioning\", \"reference\", \"eval_type\", \"ood_type\", \"level\"]\n",
    "        )[\n",
    "            [\n",
    "                \"conditioning\",\n",
    "                \"level\",\n",
    "                \"reference\",\n",
    "                \"eval_type\",\n",
    "                \"ood_type\",\n",
    "                \"Delta (Mean)\",\n",
    "                \"diff\",\n",
    "            ]\n",
    "        ]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    all_inference_mode_diffs[\"diff\"] = all_inference_mode_diffs[\"diff\"].apply(\n",
    "        lambda x: \"+\" + f\"{round(x*100, 2)}\" if x > 0 else f\"{round(x*100, 2)}\"\n",
    "    )\n",
    "    all_inference_mode_diffs[\"Delta (Mean)\"] = all_inference_mode_diffs[\n",
    "        \"Delta (Mean)\"\n",
    "    ].apply(lambda x: \"+\" + f\"{round(x*100, 2)}\" if x > 0 else f\"{round(x*100, 2)}\")\n",
    "    save_results_as_csv(\n",
    "        all_inference_mode_diffs, \"all_levels\", \"all_inference_mode_diffs\", output_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric =\"gc_success\"\n",
    "level = \"\"\n",
    "\n",
    "colors = {\n",
    "    \"aarg_colors\": [\n",
    "        \"#2F8374\",\n",
    "        \"#499385\",\n",
    "        \"#63A297\",\n",
    "        \"#7DB2A8\",\n",
    "        \"#97C1BA\",\n",
    "        \"#B1D1CB\",\n",
    "        \"#CBE0DC\",\n",
    "        \"#E5F0EE\",\n",
    "    ],\n",
    "    \"hwu_colors\": [\n",
    "        \"#970E53\",\n",
    "        \"#A42C69\",\n",
    "        \"#B14A7E\",\n",
    "        \"#BE6894\",\n",
    "        \"#CB87A9\",\n",
    "        \"#D8A5BF\",\n",
    "        \"#E5C3D4\",\n",
    "        \"#F2E1EA\",\n",
    "    ],\n",
    "    \"uoe_si_colors\": [\n",
    "        \"#004f71\",\n",
    "        \"#206583\",\n",
    "        \"#407B94\",\n",
    "        \"#6091A6\",\n",
    "        \"#80A7B8\",\n",
    "        \"#9FBDCA\",\n",
    "        \"#BFD3DB\",\n",
    "        \"#DFE9ED\",\n",
    "    ],\n",
    "}\n",
    "experiment_name = \"conditioning\"\n",
    "size = \"small-half\"\n",
    "data_home = f\"{ROOT}/data/{experiment_name}/output\"\n",
    "output_path = f\"{ROOT}/data/{experiment_name}/output/results\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = get_combined_df(data_home, level=\"\")\n",
    "plot_categories = sorted(comb_df[\"level\"].unique().tolist())\n",
    "plot_categories.extend([\"AllLevels\", \"SingleRooms\", \"Mazes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOTOLOCAL\n",
      "Done!\n",
      "GOTOOBJ\n",
      "Done!\n",
      "PICKUP\n",
      "Done!\n",
      "PICKUPLOC\n",
      "Done!\n",
      "PUTNEXT\n",
      "Done!\n",
      "PUTNEXTLOCAL\n",
      "Done!\n",
      "SYNTH\n",
      "Done!\n",
      "SYNTHLOC\n",
      "Done!\n",
      "ALLLEVELS\n",
      "Done!\n",
      "SINGLEROOMS\n",
      "Done!\n",
      "MAZES\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "inference_mode_diffs_dfs = []\n",
    "for category in plot_categories:\n",
    "    inference_mode_diffs_rtg, inference_mode_diffs_mission = plot_and_save_results(comb_df, category, metric, colors, size, output_path)\n",
    "    inference_mode_diffs_dfs.append(inference_mode_diffs_rtg)\n",
    "\n",
    "save_all_inference_mod_diffs(inference_mode_diffs_dfs, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
